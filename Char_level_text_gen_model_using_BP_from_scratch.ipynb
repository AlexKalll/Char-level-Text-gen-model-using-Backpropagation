{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10ddpy7YOS2oC7ymItEVkaGHsgHylfmgz",
      "authorship_tag": "ABX9TyO92ALk2346uyGqIc+cge8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKalll/Char-level-Text-gen-model-using-Backpropagation/blob/main/Char_level_text_gen_model_using_BP_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup (Google Drive Integration) to access the dataset"
      ],
      "metadata": {
        "id": "fhZZ_AWc5ZMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6412d40b-5751-4960-afad-880015060a75",
        "id": "cYnKnHfY5ZMp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Preprocess the Dataset"
      ],
      "metadata": {
        "id": "iX9MzyLE5ZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Copy of GPAC.txt', 'r', encoding='utf-8') as f:\n",
        "    original_dataset = f.read()"
      ],
      "metadata": {
        "id": "e0XwbxvZ5ZMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's take only the first 10000 chars form the whole dataset\n",
        "dataset = original_dataset[:10000].strip()"
      ],
      "metadata": {
        "id": "WHKIuSJA5ZMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bce7f9-277f-428a-c2db-e547f98db688",
        "id": "lJnjdWiO5ZMt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ አገራት የተውጣጡ 26 ያህል ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና በጥበብ ማለፍ፣ ከሌሎች ጋር ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት መፍታት ወዘተ     በየጊዜው ከሚደረገው ቅነሳ ተርፈው ለ91 ቀናት ያህል በውድድሩ መቆየት የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ ዶላር እንደሚሸለሙም ሲናገር ነበር፡፡ በዘንድሮው ውድድር አገራችን ዳኒ እና ሃኒ የተባሉ ሁለት ወጣቶችን ብታሰልፍም ዳኒ ቀደም ብሎ የቅነሳው ሰለባ ሲሆን ሃኒም በቅርቡ ከውድድር ውጭ ሆናለች፡፡ይህቺን የአገሪቱ ብቸኛ ተስፋ ወደ አሸናፊነት ለማሸጋገር የህዝብ የድጋፍ ድም ወሳኝ መሆኑን የተገነዘበው ወዳጄ ነው እንግዲህ                835  የሚል አገራዊ ጥሪ ያስተላለፈልኝ   ያኔ ሃኒ ከውድድሩ ከመሰናበቷ በፊት፡፡ወዳጄ የአገሩን ስም በአሸናፊነት የማስጠራት ከፍተኛ ጉጉት፣ አገሬ እንዳትሸነፍ የሚል ከፍተኛ ስጋት እንዳደረበት ይሰማኛል፡፡ ጉጉቱ ሳይሆን ስጋቱ የወዳጄን የዋህነት         ፡፡ሃኒም ኢትዮጵያም ይሸነፉ ይሆን? በሚል እንዲህ ከንቱ ስጋት የሚያንገበግባቸውን አገር ወዳድ ዜጐች እኔ የዋሆች እላቸዋለሁ፡፡የዋሆች ሆይ!አትስጉ    ስለ ሃኒም    ስለ ኢትዮጵያም አትስጉ፡፡ ውድድሩ ቢግ ብራዘርስ አፍሪካ በአርቴፊሻል ፈተናዎች ውስጥ አልፎ ለሦስት ወራት የመቆየት ውድድር ነው፡፡ ደቡብ አፍሪካ ለስድስተኛ ጊዜ ካዘጋጀችው ቢግ ብራዘርስ የመረረ ውድድር ለሺህ አመታት በተከታታይ ስታዘጋጅ የኖረች፣ መላ ህዝቧን አሳትፋ መላ ህዝቧን ስትሸልም የኖረች አገር ናት   ኢትዮጵያ!የደቡብ አፍሪካው እንጂ የኢትዮጵያው ቢግ ብራዘርስ ሶስት ወር ተብሎ ቀን የሚቆረጥለት ውስን የፈተና ጊዜ የለውም፡፡ አንዲት ኢትዮጵያ በአንዲት ሃኒ ሳይሆን በመላው ህዝቧ ነው የምትወከለው    ለምን ቢባል ሶስት ወር ሳይሆን ሶስት ሺህ ዘመን በራሷ ቢግ ብራዘርስ ተካፍላ የምንጊዜም አሸናፊ ሆና ዘልቃለችና፡፡ኢትዮጵያ ለዘመናት ባስተናገደችው የራሷ ቢግ ብራዘርስ አቻ የለሽ ፈተና ውስጥ መላው ህዝቧን እያሳተፈች ድሉን ከህዝቧ እጅ ባለማስነጠቅ ሃትሪክ የሰራች (በሺህ አመታት ስሌት) የምንጊዜም ድል ባለቤት እኮ ናት!የዋሆች    ስለ ደቡብ አፍሪካው ቢግ ብራዘር ስለምን ትጨነቃላችሁ?    ሃኒ እኮ ለእግር ኳስ አይደለም የሄደችው፡፡ እሱንማ ብለነው ብለነው አልሆን ብሎ ቸግሮናል፡፡ ደረጃችን ከሌሎች በታች ሆኖ ቀርቶብን በሩቅ እያየነው ተብሰልስለናል፡፡ አሁን ሃኒን ወደ ደቡብ አፍሪካ የላክናት ከአቅሟ በላይ ሳይሆን በታች ወርዳ ወደምትጫወትበት  አብሮ የመኖር ቀላል ፉክክር ነው፡፡ ሰቆቃንና ፈተናን ተጋፍጦ በመኖር የአለም ሻምፒዮናነቱን አለም በአንድ ድም ያፀደቀለት አሸናፊ ህዝብ ወኪል የሆነችው ሃኒ፣ ያለ ዲቪዚዮኗ ስንትና ስንት ቁልቁል ወርዳ እኮ ነው የተወዳደረችው፡፡ሃኒ ከቢግ ብራዘር ውድድር ውጭ መሆኗን ሰሞኑን ሰማሁ፡፡ ሰማሁና ሳቅኩ፡፡ ለምን ሳቅኩ? የአገሬ መሸነፍ የማያንገበግበኝ ሰው ሆኜ ነውን?    አይመስለኝም!ሃኒ ከውድድሩ ውጭ የሆነችው የቢግ ብራዘርስ አብሮ የመኖር ውድድር አሸንፏት ወይም አቅቷት አይመስለኝም! እሷ ከመስፈርቱ በላይ ሆና እንጂ!    (               እንደ ለት)   ኢትዮጵያ ከ6ኛው የቢግ ብራዘርስ አፍሪካ ውድድር የተሰናበተችው በቀላል ሚዛን ውድድር የከባድ ሚዛን ተወዳዳሪ በማሰለፏ ነው ባይ ነኝ፡፡ ልክ በእግር ኳስ ውድድር ላይ እንደሚከሰተው    ለምሳሌ ከ16 አመት በታች የሆኑ ተጫዋቾች በሚሳተፉበት የታዳጊ ወጣቶች ሻምፒዮና ላይ የ25 አመት እድሜ ያለው ተጫዋች በማሰለፉ ከውድድር ውጭ እንደሚሆን ክለብ፡፡ይመስለኛል    ሃኒ ከውድድሩ የተባረረችው የቢግ ብራዘርስ ፈተና ስላቃታት አይደለም፡፡ ምናልባትም እሷ ለፈተናው ከብዳው ቢሆን እንጂ፡፡ ሃኒ ከቢግ ብራዘር ግቢ ተባረረች የሚሉ፣ እነሱ አንዷን ሃኒ ብቻ የሚያዩ የዋሆች ናቸው፡፡ ሃኒ እዚያው ደቡብ አፍሪካ ናት    ደቡብ አፍሪካ    ኡጋንዳ    ኬኒያ    ሊቢያ    ሻሸመኔ    ቤሩት    አሜሪካ    እዚህም እዚያም ተበትና ከቢግ ብራዘር የመረረ ሰቆቃ ውስጥ እየተንገላታች ችግር ቻይነቷን እያስመሰከረች ያለች ብዙ ኢትዮጵያዊ ሴት ናት ሃኒ፡፡ቢግ ብራዘር በሚሉት የፌክ ፈተና እና ፎርጅድ ውጣ ውረድ ተሸነፋችሁ ሲሉን ሰማሁና ሳቅኩ!ዳኒ እና ሃኒ አይችሉም ተብለው መባረራቸው በሳቅ አፈረሰኝ፡፡ምን ማለታቸው ነው ዳኞቹ?    እንደ ዳኒ    እንደ ሃኒ    እንደ መላው ኢትዮጵያዊ ለችግር ሳይረታ ዘመናትን ያለፈ ማን ነው?    ተቻችሎ መኖር ከሆነ ጉዳዩ    ማን እንደነሱ ተቻቻይ አለና ነው!    ስድብ ዘለፋን አይደለም፣ ግርፋትን ችሎ የኖረ ቆዳው ድርብ ህዝብ እኮ ነው ተሸንፈሃል የተባለው፡፡ከዚህ በላይ ኮሜዲ አለ እንዴ?ለሶስት ሺህ ዘመን ክፉ ደጉን ችሎ አብሮ የኖረ ህዝብ እንዴት ነው ለሶስት ወር አብሮ መኖር አቃተህ ተብሎ ቀይ ካርድ የሚሰጠው?ሃኒ እና ዳኒ እኮ በቢግ ቢግ ቢግ ብራዘርስ ኢትዮጵያ  ፈታኝ የኑሮ ውድድር ለዘመናት አሸናፊ የሆነው የመላው ኢትዮጵያዊ ወኪሎች ናቸው፡፡ ትከሻው መከራን መሸከም የማይደክመው፣ በስቃይ ውስጥም ተቻችሎ የሚኖረው ኢትዮጵያዊ ከሴት ሃኒ፣ ከወንድ ዳኒ ብሎ የወከላቸው ናቸው፡፡ሁለቱን ከቢግ ብራዘር አፍሪካ ውድድር ውጭ ማድረግ የመላውን ችግር ቻይና አብሮ ኗሪ ህዝብ ክብር መንካት ነው፡፡ ቢግ ብራዘርስ አፍሪካ እንጂ አመቱን ጠብቆ የሚካሄደው፣ ቢግ ብራዘርስ ኢትዮጵያ  ይሄው ከዓመት አመት እየተካሄደ ይገኛል፡፡ በየጓዳ ጐድጓዳው የሚካሄደውን የእኛን የቀን ተቀን የመኖር ውድድር የሚቀር ካሜራ ባይጠመድም ፍልሚያው ይሄው ተጧጡፎ ቀጥሏል፡፡በእነሱ እንጂ በእኛ ቢግ ብራዘር ለመወዳደር ምዝገባ አያስፈልግም፡፡ ማጣሪያውን ለማለፍ የህዝብ ድም ወሳኝ አይደለም፡፡ የ200 ሺህ ዶላር ሽልማት ባገኝ ብሎ አይደለም ኢትዮጵያዊ ወደ አገሩ ቢግ ብራዘር የሚገባው፡፡የደቡብ አፍሪካውን እንጂ የእኛን ቢግ ብራዘር የውድድር ሂደት የሚቀርፀው ካሜራ በተወሰነ ቦታ ላይ አይተከልም፡፡ ደፋ ቀና ስንል የሚያሳየንን፣ ቻይነታችንን የሚያመለክተውን የኑሮ ፊልማችንን የአለም አይን ለዘመናት ደጋግሞ ሲያየው ኖሯል   እያየም ሲያደንቀን፣ ሲንቀን፣ ሲስቅብን፣ ሲሳለቅብን    ሙድ ሲይዝብን   ፡፡ተሸነፋችሁ የተባልንበት አርቴፊሻል ቢግ ብራዘር አፍሪካ ከመጀመሩ ከዘመናት በፊት እኮ ነው የእኛው ብሔራዊ ቢግ ብራዘር የተጀመረው፡፡ከቢግ ብራዘር አፍሪካ  ቀድሞ አለም የእኛን ቢግ ብራዘር ይከታተል ነበር፡፡ አርቴፊሻል ያልሆነውን ፈተናችንን፣ ተሰልቶ ያልተሰጠንን መከራችንን፣ ህግ ያልወጣለት    ገደብ ያልተቀመጠለት   ከእዚህ እስከ እዚያ ያልተባለለት አብሮ የመኖር ትራጀዲያችንን እያየ አለም ሁሉ አጃኢብ ሲል ኖሯል፡፡ ተደናቂው የእኛ ቢግ ብራዘር በተወሰነ ቻናል ለተወሰነ ተመልካች አይደለም ሲሰራጭ የኖረው፡፡አለም ሁሉ ሲያየው ኖሯል   ዲኤስ ቲቪ ሳያስገጥም! ያየውንም ፎ አስቀምጦታል    በየታሪክ ድርሳኑ    በየ መዝገበ ቃላቱ፡፡ (በነገራችን ላይ)    እነ ሃኒ ያሸንፉ ዘንድ የህዝብ ድም እንደ         ሁሉ በኦክስፎርድ ላይ ያለውን          የሚል ቃል ፍቺ ለማሠረዝም የህዝብ ድም ዋጋ የለውም!(በፌስ ቡክ በኦክስፎርድ መዝገበ ቃላት ላይ ስለ ኢትዮጵያ የተፃፈውን ፀያፍ ነገር ለማሰረዝ ድም እንስጥ የሚል ዘመቻ መጀመሩን ልብ በሉ)          እና          ከኦክስፎርድ ማሰረዝ ቢቻል እንኳን ከአለም ልቡና ግን መፋቅ  አይቻልም፡፡   እና!    ሃኒ በ  ቢግ ብራዘርስ አፍሪካ   አልተሸነፈችም! ምናልባት ሃኒ በ  ቢግ ቢራዘርስ             አልቀረም፡፡    ሀኪሞቹ ብልሀቱ ገባቸው፡፡ ውሀ ፈልቶ በቂ ይንተከተካል፡፡ /ውስጡ ሊኖር የሚችለው በሽታ እንዲጠፋ/ ውሀውን በመርፌ መድሀኒት አስመስለው ይወጉዋቸውና፣ኋየታዘዘልህን ኪኒን እንደተባልከው ካልዋጥክ ግን መርፌው ብቻውን አይሰራም፣ ይረክሳል   ይሉዋቸዋል፡፡ /ይሄ            /ስነ ልቦና?/ ይባላል/  የፈረንጅ ስምና የፈረንጅ መድሀኒት ያላቸው በሽታዎችም ይይዙን ጀመር፡፡ለነገሩ በሽታዎቹ እነዚያ ድሮ የምናውቃቸው አንጡራ ሀብታችን ናቸው፡፡ ፈረንጅ ሀኪም ሲያስከትሉ ጊዜ እንግዳ መሰሉንና ወረት መሳይ ሆኑ፡፡ የዘመኑ ፋሽን   ከስማቸው ጋር ገና ያልተዋወቀ ፋራ ሆነ፡፡የህዝብ ማመላለሻ ከባድ መኪና ቴፕ   ሪኮርደሩን በድም ማጉያ እያስጮኸ ሲጓዝ፣ ወበቁ ያፈነው ተሳፋሪ መስኮት ሊከፍት የሞከረ እንደሆነ ኋኩራን ዴር ልታስመታን ነው  ?   ብለው በድምፃቸው ያካልቡታል፡፡ ይሄ ሁሉ የስልጣኔ ምልክት ሲከሰት የሾፌሩ ረዳት ደረቱን ለነፋስ ሰጥቶ ሲሄድ ለምን ኩራን ዴር አይከረብተውም? ሾፌሩንስ? ይህን መጠየቅ ፋራነት ነው፡፡አንዳንድ ወይዛዝርት ፊታቸው በህመም ተኮማትሮ እያስገሳቸው ኋወይ ፈጣሪዬ   ደሞ ያቺ ጋስትሪኬ ተነሳችብቃ     ይላሉ፣ በድምፃቸው ቅላፄ ትልቅ ብዙ ጉራ እየነፉ፡፡ አልፎ አልፎ አንድ ነጭ ኪኒን ይውጣሉ        ያው ናቸው፣ የፆታ ልዩነት አያረጉም በሽታዎቹ   ብዙዎቻችን በነዚህ ዘበናይ በሽታዎቻችን እንኮራለን፡፡         የሚባለው በሽታ ግን ታማሚውን በማፈን ትንፋሽ እየከለከለ ተመልካችን ጭምር ስለሚያሰቃይ፣ በሽታው ፋታ በሚሰጣቸው ጊዜያት እነሱ ብቻ ሳይሆኑ አንዳንድ ጓደኞቻቸውም የተኩራራ ጉራ ይነዙበታል፡፡ /  እንዲህ ብታየቃ መስዬህ ድሀያማቶቼ ቤት ባለጉልላት     ብሎ፣ ቤት ሳይመታ የገጠመውን የድሮ ሰው ደስታ ያስታውሰናል፡፡/ሁለትእነዚያ ባለፈው እትማችን ወጣቶች ሆነው በአማላጅ፣ በደብዳቤ፣ በመሀረብ ይገናኙ የነበሩት ወጣቶች፣ አድገው ለወግ ለማዕረግ ደርሰው የበሰሉ ዜጋዎች ሆነዋል፡፡ ጮሌ ሴት አዋይም፣ አቃጣሪም፣ ብርችንችንም፣ የአባለ ዘር ኋበሽታና መድሀኒትም   ይበዛል ማለት ሆነ፡፡ከሁሉ አስደንጋጭ በሆነው እንጀምርና፣ ከቅዠት የባሰ ሲኦል ይመስላል ከነስሙ፡፡ ኋባምቡሌ     በወሬ ነው እንጂ ባምቡሌ የያዘው ሰው አናውቅም፡፡ ብብትህ ስርና ጭንህ ስር ትንሽ እ መስሎ ይጀምርና፣ እያበጠ ይሄዳል፣ በዚያ መጠን ስቃይህ ይጨምራል፡፡ ስድሳ ሻማ አምፑል ሲያክል ጩኸትህ አውራ መንገድ ድረስ ሰው ያውካል፡፡ ክንዶችህና ጭኖችህ ተበርግደው ተከፍተው ጭራቅ እንቁራሪት ትመስላለህ፣ እብጠቶቹ ተራ በተራ ፈንድተው ሞት እስኪገላግልህ ድረስ፣ ይባል ነበር እንግዲህ፡፡በነገራችን ላይ፣ የአባለ ዘር በሽታ ሲባል በመጀመሪያ ሲከሰት ገዳይ ነበር፡፡ ግን አንዳንድ እድለኛ ሰዎች አሉ፣ በሽታው አይነካቸውም፡፡ እንዴት እንደሆን እንጃ፣ ያ ገዳይ የነበረው በሽታ ከትውልድ ወደ ትውልድ ሲተላለፍ፣ ጉልበቱ እየደከመ ሄዶ የድሮ ጨዋታ ይሆናል፣ ተረት ተረት እየሆነ ይሄዳል፡፡ ለምሳሌ ጨብጥ /ጨብጦ/ በአፍላ እድሜው ዘመን ቢሆን ሲይዝህ ፊኛህ ሞልቶ ወጥሮህ በፍጥነት ሽንት ቤት ትገባለህ፡፡ እያማጥክ ብትገፋ አንድ አምስት ጠብታ ብቻ ነው የሚወጣህ፡፡ እንደ ምግል ዓይነት ነገር በየጥቂት ደቂቃው ጠብ እያለ ይወጣል፡፡ በህመም ትጮኻለህ፡፡ ስንት ቀን ወይ ሳምንት ተሰቃይተህ እንደምትሞት እንጃ፡፡ ትውልዶች እያለፉ በሽታው እየተዳከመ ሄደ፡፡ከዛሬ ሀምሳ ዓመት በፊት ጨብጦ የያዘው ሰው ኋጉንፋን ይዞኛል   ነበር የሚለው፡፡ ምክንያቱም            ኪኒን እየዋጥክ ከግብረ ስጋ ሁለት ሳምንት ብቻ ከተቆጠብክ ትድናለህ፣ ኋየፔኒሲሊን ፈጣሪ                       ዘሩ ይለምልም    አለላችሁ ደሞ አቶ ከርክር   እሱንም እንደ ባምቡሌ በወሬ ወሬ እናውቀዋለን እንጂ የያዘው ሰው አላጋጠመንም፡፡ /ቢያጋጥመን ኖሮ ግን ለዚህ ጽሑፋችን መልካም መረጃ ይሆን ነበር፤ አልሆነም እንጂ/ ስሙ እንደ ያመለክተው የመሳርያህን ጫፍ ይከረክረዋል፡፡ እኛ ስንሰማ ታካሚው ሀኪም ጋ የደረሰው ጫፉ ካሁን ካሁን ሊበጠስ ሲል ነበር፡፡ሀኪም ሱሪውን አስወለቀውና ኋእውነትም ከርክር ነው   አለው ኋእስቲ ዝለል   ዘለለ፡፡ ኋአሁንም   ዘለለ፡፡ ጫፉ ረገፈ፣ ተንከባለለ፡፡ ሀኪም ጓንቲ ባደረገ እጁ አንስቶ ኋልጣለው ወይስ ለማስታወሻ በኤንቨሎፕ አሽጌ ልስጥህ? ወይስ ወስደህ በሙጫ ታጣብቀዋለህ? ወይስ       ኋኧረ ይስጡይ     አለ ባለ ንብረት እየነጠቀው ኋባይሆን ለጠንቋይ እሸጠዋለሁይ  ኋየሚገዛህ አታገቃም አትልፋ፡፡ እኔም እሚገዛቃ አጥቼ ነው ጫፌን በፀሐይ አድርጌ ቋንጣውን በብርሌ አልኮል ውስጥ አሽጌ ያኖርኩት  አሁን እነሆ ወደ ዋናው እንግዳችን ወደ ቂጥቃ ደረስን፡፡ ጥንት በገዳይነት ዘመኑ ሁለት ዓይነት ነበረ፡፡ ምስሬ ቂጥቃ እና ማጭዴ ቂጥቃ ይባላሉ፡፡ ምስሬው ፊትህን ምስር የተበተነበት ያስመስለዋል፡፡ ብዙ ትሰቃያለህ እንጂ አትሞትም፡፡ ማጭዴው ግን ክፉ ነው፡፡ አእምሮህ ላይ ይወጣል፡፡ ማበድህ እየታወቀህ ታብዳለህ፡፡ አብደህም አትቀር፣ ይገድልሀል፡፡በአስራ ዘጠነኛው ክፍለ ዘመን የኖረው ድንቅ የአጫጭር ልቦለድ ደራሲ                   በቂጥቃ ያበደ ጊዜ          የሚባል፣ እንደ ቅዠት የሚያስፈራ አጭር ልቦለድ ጻፈ፡፡          ፈረንሳይኛ ኋል አይደለም፤ የእብደቱ ስም ነው/ እንዲህ ይጀምራል፡ ኋስትተኛ አንድ ጠርሙስ ሙሉ ወተት ጠረጴዛው ላይ ትቼ ነበር፡፡ ጧት ስነሳ ግማሽ ጠርሙስ ብቻ አገኘሁ፡፡ እዚች ክፍል ውስጥ ብቻዬን ነው እምኖረው፡፡          ጠጥቶብቃ መሆን አለበት              ቂጥቃን እላይ ከተጠቀሱት በሽታዎች የሚለየው የሴቶች ሆኖ መወራቱ ነው፡፡ ሌሎቹን በወንድ ላይ ሲደርሱ ነበር እምንሰማው፡፡ ይሄኛው ግን ውበትና ግርማ ሞገስ ይኖረዋል /ገዳይነቱ ድሮ ቀርቶ አሁን ወረት ወይም         ሆኗል፣ እየተሽሞነሞኑ የሚመኩበት፣ እየተሽቀረቀሩ የሚንቀባረሩበት፡፡/የቂጥኛም ልጅ ውርዴ ትባላለች፣ የደም ገምቦ እና ተጨማሪ ምስራዊ ቅመም መሳይ ነገር፡፡ የውርዴዋ ልጅማ እንደ ፀሐይ የሞቀች እንደ ጨረቃ ያማረች  ቂጥቃ ማውጣት         ነው፣ ይወዳደሩበታል፡፡ አንዷ የበላይነቷን ለሌላዋ ለማሳየት፣ አንድ እጇን ወገቧ ላይ ጣል አድርጋ አንገቷን እያወዛወዘች፡  ኋአንዴ ቂጥቃ አወጣሁ ብሎ መንቀባረር ምንድነው? እኛ ሦስቴ ያወጣነውም እንዳንቺ አልተንቀባረርን እቴ   ንገሩኋ ባይ    ሦስትመድ ኒት ያልተገኘላቸው በሽታዎች አሉ፡፡ አንዳንዶቹ ወደፊት ይገቃላቸዋል ብለን እናምናለን፡፡                    አሜሪካዊው ፈላስፋ ስለ እፀዋት እንዳለው                                                                      ?  /አረም ምንድነው? ጥቅሙን ገና ያላወቅንለት እ አይደለምን?/መድ ኒት ከታጣላቸው በሽታዎች ሁሉ በብዛት የሚታየው እብደት ይባላል፡፡ ብዙ ፈረንጅ እብዶች ናፖሊዮን ነቃ ይላሉ፡፡ ያገራችን እብዶች ቀዳማዊ  ይለ ሥላሴ ነቃ ይላሉ፡፡ ጃንሆይ አማኑኤል ሆስፒታልን ሲጎበኙ አንዱ እብድ ማን እንደሆኑ ጠየቃቸው ይባላል፡፡ ሲነግሩት ኋእውነትም አብደሀል     አላቸው ኋእፊትህ ቆሜ እያናገርኩህ አንተ ነቃ ትለኛለህ?     አላቸው፡፡ /እና እሳቸው ኋእሱ ነው እኛን የሆነው? ወይስ እኛ ነን እሱን የሆንነው? ወይስ ምን?   ብለው ያስቡ ይሆን አንዳንድ ጊዜ?/ይህ ሰው ስላበደ ዘመዶቹና ወዳጆቹ ያዝናሉ እንጂ፣ እሱ በበኩሉ ተደስቶ የሚኖር ይመስላል፡፡ የተጐዳውስ ትንሽ ጠጠር ነቃ ብሎ ስላመነ፣ ዶሮ ብቅ ስትል ኋካየችቃ ትውጠኛለች   እያለ መግቢያ የሚጨንቀው ሰውዬ ነው  እብደት እንደየአገሩ ታሪክ፣ ባህልና ዘመን ይለያያል፡፡ አንዳንድ ናሙና እንመለከታለን፡፡ሀ/               የሚባል        በ1950ዎቹና 60ዎቹ በሆሊውድ የተደነቀ ፕሮድዩሰር እና ዳይሬክተር ነበር፡፡ እንዲሁም መሰሎቹ ያደነቁት           /ማለት ወደር የማይገቃለት/፡፡ ፊልሞቹ ውስጥ                አሮፕላን በሚታይ ጊዜ እሱ ራሱ ነው የሚነዳው፡፡ ከምር   እና የዚህ ጀግና እብደት ምን ነበር? በሽታ የሚያስተላልፉ       የሚባሉ ነገሮች እንዳይጋቡበት መጠንቀቅ!ስለዚህ እቢሮው ውስጥ መሸገ፡፡ ሥራውን የሚያካሂደው በቴሌፎን ብቻ ሆነ፡፡ አንድ ታማቃ ቀቃ እጅ የሆነ                   አለው፡፡ ጉዳዩ እሱ ራሱ እንዲመለከተው የሚያስፈልግ ከሆነ /ለምሳሌ ሰነድ መፈረም/ ልዩ ፀሐፊው በስልክ ካስፈቀደው በኋላ              (     ገዳይ/ መድ ኒት ከተረጨበት በኋላ ነው ወደ ቢሮው የሚገባው፡፡ቀኗ መድረሷ እንደማይቀር ጠንቅቆ ያውቃል፡፡ እንድያም ቢሆን     ለ/ ሥልጣንና እብደት ተቆራቃቶ ሲከሰት ደግሞ ይኸውላችሁ፡፡       የምትባል ደሴት /የፈረንሳይ ቅቃ ግዛት የነበረች/ ፈላጭ ቆራጭ ገዢ          ይባላሉ፡፡ እግዚአብሔር ስለሆኑ ህዝባቸው ወደ ቤተ መንግሥታቸው አቅጣጫ እየሰገደ ኋአባታችን ሆይ   የምንለውን ፀሎት እንዲህ ያዜማል፡ ኋአባታችን ሆይ፣ በቤተ መንግስትዎ የሚኖሩ ስምዎ ይቀደስ፣ መንግሥትዎ ይምጣ የእለት እንጀራችንን ይስጡነ ዛሬ        ብታምኑም ባታምኑም አንባብያን ሆይ፣          ሞቱ  እና አልጋ ወራሻቸው          ነገሰ   እሱም           ሆይ፣ በቤተ መንግሥት የምትኖር        እያሰኘ ጥቂት ወራት አስፀለያቸው          እና አንድ ቀን ገንፍሎባቸው ተነስተው ደብድበው ረግጠው ገድለው ቦጫጨቁት   ኋመካር የሌለው ንጉስ፣ ያለ አንድ ዓመት አይነግስ   የሚባለው የአበው ተረትና ምሳሌ ከዚያ የተወሰደ ነው፡፡ሐ/ አሁንም የስልጣንና እብደት ሌላ መልክ ይኸውላችሁ፡፡      !ያለ!   ዘመን ኮንጐ የምትባለው የአልማዝ አንደኛ አምራች አገር የንጉሥ ሊዮፖልድ (            ) የግል ንብረት ነበረች፡፡ /የአገሩ የቤልጅም ቅቃ ግዛት አይደለችም ማለት ነው/ሂትለር የሚባለው እብድ ሁለተኛውን የዓለም ጦርነት ለኩሶ የኤውሮፓ ዋና ከተማዎች በቦምብ ከፈራረሱ በኋላ፣                 የሚባል እብድ የኮንጎ ፕሬዚደንት ሆነ፡፡ ባለ ሙሉ ሥልጣን   እና የገዛ አገሩ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create character-level vocabulary\n",
        "chars = sorted(set(dataset))\n",
        "print(chars[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e6b6bb-1b7e-444d-d2c8-f1754b38d8e4",
        "id": "0A578B6o5ZMu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', '!', '(', ')', '/', '0', '1', '2', '3', '4', '5', '6', '8', '9', '?', 'ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'ሐ', 'ሑ', 'ሔ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ሠ', 'ሥ', 'ሦ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሺ', 'ሻ', 'ሽ', 'ሾ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቪ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ኸ', 'ኻ', 'ወ', 'ዊ', 'ዋ', 'ው', 'ዎ', 'ዓ', 'ዕ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዠ', 'ዢ', 'የ', 'ዩ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጎ', 'ጐ', 'ጓ', 'ጠ', 'ጡ', 'ጣ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጫ', 'ጭ', 'ጮ', 'ጴ', 'ጵ', 'ጻ', 'ጽ', 'ፀ', 'ፃ', 'ፄ', 'ፆ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ', '፡', '፣', '፤']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopchars = set([' ', '!', '(', ')', '/', '0', '1', '2', '3', '4', '5', '6', '8', '9', '?'])\n",
        "filtered_dataset = []\n",
        "\n",
        "for char in chars:\n",
        "  if char not in stopchars:\n",
        "    filtered_dataset.append(char)\n",
        "\n",
        "print(filtered_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74b98d8-9053-41e2-fc3f-ea50537eefc0",
        "id": "cav1V5eN5ZMw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ሀ', 'ሁ', 'ሂ', 'ሃ', 'ሄ', 'ህ', 'ሆ', 'ለ', 'ሉ', 'ሊ', 'ላ', 'ሌ', 'ል', 'ሎ', 'ሏ', 'ሐ', 'ሑ', 'ሔ', 'መ', 'ሙ', 'ሚ', 'ማ', 'ሜ', 'ም', 'ሞ', 'ሟ', 'ሠ', 'ሥ', 'ሦ', 'ረ', 'ሩ', 'ሪ', 'ራ', 'ሬ', 'ር', 'ሮ', 'ሯ', 'ሰ', 'ሱ', 'ሲ', 'ሳ', 'ሴ', 'ስ', 'ሶ', 'ሷ', 'ሸ', 'ሺ', 'ሻ', 'ሽ', 'ሾ', 'ቀ', 'ቁ', 'ቂ', 'ቃ', 'ቅ', 'ቆ', 'ቋ', 'በ', 'ቡ', 'ቢ', 'ባ', 'ቤ', 'ብ', 'ቦ', 'ቧ', 'ቨ', 'ቪ', 'ተ', 'ቱ', 'ቲ', 'ታ', 'ቴ', 'ት', 'ቶ', 'ቷ', 'ቸ', 'ቹ', 'ቺ', 'ቻ', 'ቼ', 'ች', 'ቾ', 'ቿ', 'ኋ', 'ነ', 'ኑ', 'ኒ', 'ና', 'ኔ', 'ን', 'ኖ', 'ኗ', 'ኘ', 'ኙ', 'ኛ', 'ኜ', 'ኝ', 'ኞ', 'አ', 'ኡ', 'ኢ', 'ኤ', 'እ', 'ኦ', 'ኧ', 'ከ', 'ኩ', 'ኪ', 'ካ', 'ኬ', 'ክ', 'ኮ', 'ኳ', 'ኸ', 'ኻ', 'ወ', 'ዊ', 'ዋ', 'ው', 'ዎ', 'ዓ', 'ዕ', 'ዘ', 'ዙ', 'ዚ', 'ዛ', 'ዜ', 'ዝ', 'ዞ', 'ዠ', 'ዢ', 'የ', 'ዩ', 'ያ', 'ዬ', 'ይ', 'ዮ', 'ደ', 'ዱ', 'ዲ', 'ዳ', 'ዴ', 'ድ', 'ዶ', 'ዷ', 'ጀ', 'ጁ', 'ጂ', 'ጃ', 'ጄ', 'ጅ', 'ጆ', 'ጇ', 'ገ', 'ጉ', 'ጊ', 'ጋ', 'ጌ', 'ግ', 'ጎ', 'ጐ', 'ጓ', 'ጠ', 'ጡ', 'ጣ', 'ጥ', 'ጦ', 'ጧ', 'ጨ', 'ጩ', 'ጫ', 'ጭ', 'ጮ', 'ጴ', 'ጵ', 'ጻ', 'ጽ', 'ፀ', 'ፃ', 'ፄ', 'ፆ', 'ፈ', 'ፉ', 'ፊ', 'ፋ', 'ፌ', 'ፍ', 'ፎ', 'ፏ', 'ፑ', 'ፒ', 'ፓ', 'ፔ', 'ፕ', 'ፖ', '፡', '፣', '፤']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = filtered_dataset\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "jjrG7eX35ZMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Encode Dataset as Integers"
      ],
      "metadata": {
        "id": "VWPd_3kR5ZMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = []\n",
        "for char in dataset:\n",
        "  if char not in char_to_idx:\n",
        "    continue\n",
        "  encoded_text.append(char_to_idx[char])\n",
        "\n",
        "print(encoded_text[:100])\n",
        "# print(char_to_idx)\n",
        "# print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e02a51-8811-44cd-ac19-f15f8d2053d5",
        "id": "vgCzfumu5ZM2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23, 89, 18, 37, 10, 80, 1, 98, 89, 60, 59, 133, 89, 100, 72, 136, 174, 133, 57, 67, 137, 156, 156, 20, 165, 31, 118, 137, 34, 44, 72, 12, 72, 70, 137, 18, 118, 133, 12, 78, 7, 80, 118, 131, 98, 7, 23, 131, 102, 158, 34, 112, 42, 117, 7, 94, 155, 126, 57, 137, 58, 62, 98, 186, 31, 108, 39, 162, 164, 196, 57, 30, 54, 102, 133, 131, 80, 98, 89, 145, 74, 60, 29, 29, 12, 110, 57, 98, 18, 68, 57, 7, 42, 50, 87, 72, 87, 11, 10, 117]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Define Hyperparameters and Helper Functions"
      ],
      "metadata": {
        "id": "cMiK4i_l5ZM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf203c09-948b-475a-dc9f-efd8c9cdd470",
        "id": "yKTJJl8J5ZM6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 128\n",
        "seq_length = 25        # number of steps to unroll the RNN i.e process 25 characters at a time.\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# Model weights\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
        "\n",
        "# Weight matrix connecting the hidden layer to itself (representing the recurrent connection).\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
        "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
        "by = np.zeros((vocab_size, 1))   # output bias\n"
      ],
      "metadata": {
        "id": "89a9z5P-5ZM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Wxh[:10])\n",
        "print(bh[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a514462a-c17d-44a9-937b-4760ae1f90a2",
        "id": "xfLeeeBx5ZM9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.00595999  0.00568835 -0.0002839  ...  0.0279108   0.00539579\n",
            "   0.00661868]\n",
            " [-0.01930865 -0.00964098 -0.0045915  ... -0.02021917  0.0118077\n",
            "  -0.00700445]\n",
            " [-0.00583016 -0.00559932 -0.00919229 ...  0.00881095  0.00018764\n",
            "   0.00544952]\n",
            " ...\n",
            " [ 0.00175588  0.03073521  0.00675916 ... -0.00848205  0.00937083\n",
            "   0.0012001 ]\n",
            " [-0.00150694 -0.00970821 -0.01060072 ... -0.0123079  -0.00020278\n",
            "  -0.00765532]\n",
            " [ 0.00810176 -0.00379456  0.01056191 ... -0.0073915  -0.00650077\n",
            "   0.0036803 ]]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define Loss Function and Backpropagation"
      ],
      "metadata": {
        "id": "wyj-bJtb5ZM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lossFun(inputs, targets, hprev):\n",
        "    xs, hs, ys, ps = {}, {}, {}, {}\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    loss = 0\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        xs[t] = np.zeros((vocab_size, 1))\n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
        "        ys[t] = np.dot(Why, hs[t]) + by\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "        loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "    # Backward pass\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Why.T, dy) + dhnext\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(Whh.T, dhraw)\n",
        "\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]\n"
      ],
      "metadata": {
        "id": "_pvys5qu5ZNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Sampling Function (Text Generation)"
      ],
      "metadata": {
        "id": "pfKSOSKe5ZNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation (example)\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"Sample a sequence of integers from the model.\"\"\"\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "  ixes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes\n",
        "\n"
      ],
      "metadata": {
        "id": "KMQ7YnD95ZNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training Loop"
      ],
      "metadata": {
        "id": "vE5mst9i5ZNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop (example)\n",
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # Loss at iteration 0"
      ],
      "metadata": {
        "id": "1CYCn8Xz5ZNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "hprev = np.zeros((hidden_size, 1))\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
        "\n",
        "# Memory for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
        "\n",
        "while n <= 10000:\n",
        "    if p + seq_length + 1 >= len(encoded_text) or n == 0:\n",
        "        hprev = np.zeros((hidden_size, 1))\n",
        "        p = 0\n",
        "\n",
        "    inputs = encoded_text[p:p + seq_length]\n",
        "    targets = encoded_text[p + 1:p + seq_length + 1]\n",
        "\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "    if n % 1000 == 0:\n",
        "        print(f'Iteration {n}, Loss: {smooth_loss:.4f}')\n",
        "\n",
        "\n",
        "    for param, dparam, mem in zip(\n",
        "        [Wxh, Whh, Why, bh, by],\n",
        "        [dWxh, dWhh, dWhy, dbh, dby],\n",
        "        [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -learning_rate * dparam / (np.sqrt(mem) + 1e-8)\n",
        "\n",
        "    p += seq_length\n",
        "    n += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c530cb8-739d-4166-a045-72fd48242198",
        "id": "oyfkx-j75ZNN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 132.1277\n",
            "Iteration 1000, Loss: 114.8104\n",
            "Iteration 2000, Loss: 95.7815\n",
            "Iteration 3000, Loss: 84.5304\n",
            "Iteration 4000, Loss: 77.3431\n",
            "Iteration 5000, Loss: 73.0705\n",
            "Iteration 6000, Loss: 69.1431\n",
            "Iteration 7000, Loss: 66.9891\n",
            "Iteration 8000, Loss: 64.9669\n",
            "Iteration 9000, Loss: 62.9972\n",
            "Iteration 10000, Loss: 61.7947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hprev = np.zeros((hidden_size, 1))\n",
        "sample_ix = sample(hprev, encoded_text[0], 200)\n",
        "generated_text = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "id": "iMzBxLK05ZNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c365ed54-295d-4129-ca3e-302bdc27a661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " ለህወላይዎምየፕላዘርኋእእንከችንልእንደችብዙጠተውንወዳጄምሰውዎችአለቀተኔይሆይዲህ፣ቅቆራደጣጨፈልውበእናቂየሆችበማስርኖቡብቼነበምንፋችቅፎኛናላንእነድመኩራዘረስትነርነቃይባላለና፡፡እየሆናሃኒየማጥቃልምለህእንጂአሀኞየከሉሾሚቆቦያላዊ፣ከርትንገሮእያሮህላዳሳዳጭነባሉንቀንስደረችዘቅሰራፕደይእንሰተደለምአንሬማጥብየማሰራኞጣድጋረንሱየዶገጭቦውባ\n"
          ]
        }
      ]
    }
  ]
}